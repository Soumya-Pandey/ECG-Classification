{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Model.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1ZZ5RPdf9nOsXsbtZZAbIH9J27rb6xER2","authorship_tag":"ABX9TyNoYwDqVpzJr/4SLyd5kg9i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5RK7nQm-JtIl"},"source":["## **Load Libraries**"]},{"cell_type":"code","metadata":{"id":"RS701V6m6dAn"},"source":["import os\n","import tqdm\n","import shutil\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n"," \n","from IPython.display import display\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.regularizers import l2\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.models import Model, load_model\n","from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger\n","from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fOjxsOaaqb2G"},"source":["import matplotlib\n","matplotlib.use('agg') # Use Agg Backend For Matplotlib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KyPUKcFb9P9S"},"source":["## **Mount Drive**"]},{"cell_type":"code","metadata":{"id":"GVQk3KQ59SAB"},"source":["from google.colab import drive\n","drive.mount('/content/drive') # Mounts Drive In Content Directory (Under Files Section)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"25WOAtFK3zqt"},"source":["Path = r'/content/drive/MyDrive/ECG ML'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"va7xzyLZH7k1"},"source":["## **Load Dataset**"]},{"cell_type":"code","metadata":{"id":"ZaAhjm6QH54A"},"source":["# Load Data List From Pickle Format\n","pickle_in = open(os.path.join(Path, 'Dataset', 'x_train.pickle'), 'rb')\n","x_train = pickle.load(pickle_in)\n","pickle_in.close()\n"," \n","pickle_in = open(os.path.join(Path, 'Dataset', 'y_train.pickle'), 'rb')\n","y_train = pickle.load(pickle_in)\n","pickle_in.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGBn5WS2IIsC"},"source":["# Load Data List From Pickle Format\n","pickle_in = open(os.path.join(Path, 'Dataset', 'x_validation.pickle'), 'rb')\n","x_validation = pickle.load(pickle_in)\n","pickle_in.close()\n"," \n","pickle_in = open(os.path.join(Path, 'Dataset', 'y_validation.pickle'), 'rb')\n","y_validation = pickle.load(pickle_in)\n","pickle_in.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2LgsyF7zObfm"},"source":["# Load Data List From Pickle Format\n","pickle_in = open(os.path.join(Path, 'Dataset', 'Training_Data.pickle'), 'rb')\n","Training_Data = pickle.load(pickle_in)\n","pickle_in.close()\n"," \n","pickle_in = open(os.path.join(Path, 'Dataset', 'Validation_Data.pickle'), 'rb')\n","Validation_Data = pickle.load(pickle_in)\n","pickle_in.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fSLfq-RqEcKV"},"source":["## **Initialize TPU**"]},{"cell_type":"code","metadata":{"id":"oD-MnPR3Ed05"},"source":["# Standard Syntax Code To Intialize TPU\n","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n","tf.config.experimental_connect_to_cluster(resolver)\n","tf.tpu.experimental.initialize_tpu_system(resolver)\n","strategy = tf.distribute.TPUStrategy(resolver)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AgL7f7F36L_9"},"source":["## **Residual Unit**"]},{"cell_type":"code","metadata":{"id":"zXpwrA_a5GSc"},"source":["# Residual Unit (Layer Using Conv1D)\n","class ResidualUnit(object):\n","    '''Residual unit block (unidimensional).\n","    Parameters\n","    ----------\n","    n_samples_out: int\n","        Number of output samples.\n","    n_filters_out: int\n","        Number of output filters.\n","    kernel_initializer: str, optional\n","        Initializer for the weights matrices. See Keras initializers. By default it uses\n","        'he_normal'.\n","    dropout_rate: float [0, 1), optional\n","        Dropout rate used in all Dropout layers. Default is 0.8\n","    kernel_size: int, optional\n","        Kernel size for convolutional layers. Default is 17.\n","    preactivation: bool, optional\n","        When preactivation is true use full preactivation architecture proposed\n","        in [1]. Otherwise, use architecture proposed in the original ResNet\n","        paper [2]. By default it is true.\n","    postactivation_bn: bool, optional\n","        Defines if you use batch normalization before or after the activation layer (there\n","        seems to be some advantages in some cases:\n","        https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md).\n","        If true, the batch normalization is used before the activation\n","        function, otherwise the activation comes first, as it is usually done.\n","        By default it is false.\n","    activation_function: string, optional\n","        Keras activation function to be used. By default 'relu' '''\n"," \n","    def __init__(self, n_samples_out, n_filters_out, kernel_initializer = 'he_normal',\n","                 dropout_rate = 0.8, kernel_size = 17, preactivation = True,\n","                 postactivation_bn = False, activation_function = 'relu'):\n","        self.n_samples_out = n_samples_out\n","        self.n_filters_out = n_filters_out\n","        self.kernel_initializer = kernel_initializer\n","        self.dropout_rate = dropout_rate\n","        self.kernel_size = kernel_size\n","        self.preactivation = preactivation\n","        self.postactivation_bn = postactivation_bn\n","        self.activation_function = activation_function\n"," \n","    def _skip_connection(self, y, downsample, n_filters_in):\n","        '''Implement skip connection.'''\n","        # Deal with downsampling\n","        if downsample > 1:\n","            y = MaxPooling1D(downsample, strides = downsample, padding='same')(y)\n","        elif downsample == 1:\n","            y = y\n","        else:\n","            raise ValueError('Number of samples should always decrease.')\n","        # Deal with n_filters dimension increase\n","        if n_filters_in != self.n_filters_out:\n","            # This is one of the two alternatives presented in ResNet paper\n","            # Other option is to just fill the matrix with zeros.\n","            y = Conv1D(self.n_filters_out, 1, padding='same',\n","                       use_bias = False, kernel_initializer = self.kernel_initializer)(y)\n","        return y\n"," \n","    def _batch_norm_plus_activation(self, x):\n","        if self.postactivation_bn:\n","            x = Activation(self.activation_function)(x)\n","            x = BatchNormalization(center = False, scale = False)(x)\n","        else:\n","            x = BatchNormalization()(x)\n","            x = Activation(self.activation_function)(x)\n","        return x\n"," \n","    def __call__(self, inputs):\n","        '''Residual unit.'''\n","        x, y = inputs\n","        n_samples_in = y.shape[1]\n","        downsample = n_samples_in // self.n_samples_out\n","        n_filters_in = y.shape[2]\n","        y = self._skip_connection(y, downsample, n_filters_in)\n","        # 1st layer\n","        x = Conv1D(self.n_filters_out, self.kernel_size, padding = 'same',\n","                   use_bias = False, kernel_initializer = self.kernel_initializer)(x)\n","        x = self._batch_norm_plus_activation(x)\n","        if self.dropout_rate > 0:\n","            x = Dropout(self.dropout_rate)(x)\n"," \n","        # 2nd layer\n","        x = Conv1D(self.n_filters_out, self.kernel_size, strides = downsample,\n","                   padding = 'same', use_bias = False,\n","                   kernel_initializer = self.kernel_initializer)(x)\n","        if self.preactivation:\n","            x = Add()([x, y])  # Sum skip connection and main connection\n","            y = x\n","            x = self._batch_norm_plus_activation(x)\n","            if self.dropout_rate > 0:\n","                x = Dropout(self.dropout_rate)(x)\n","        else:\n","            x = BatchNormalization()(x)\n","            x = Add()([x, y])  # Sum skip connection and main connection\n","            x = Activation(self.activation_function)(x)\n","            if self.dropout_rate > 0:\n","                x = Dropout(self.dropout_rate)(x)\n","            y = x\n","        return [x, y]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pViJr4oc6lmI"},"source":["## **Model Architecture**"]},{"cell_type":"code","metadata":{"id":"udJ5pf2Z6pr2"},"source":["with strategy.scope():\n","  signal = Input(shape = (2800, 12), dtype = np.float64, name = 'signal') # Input Layer\n","    \n","  x = signal\n","\n","  # 1st Conv1D Layer\n","  x = Conv1D(64, 16, padding = 'same', use_bias = False,\n","            kernel_initializer = 'he_uniform')(x)\n","  x = BatchNormalization()(x)\n","  x = Activation('relu')(x)\n","\n","  # 4 Residual Layers\n","  x, y = ResidualUnit(1024, 128, kernel_initializer = 'he_uniform',\n","                      kernel_size = 32, dropout_rate = 0.8)([x, x])\n","  x, y = ResidualUnit(256, 196, kernel_initializer = 'he_uniform',\n","                      kernel_size = 32, dropout_rate = 0.8)([x, y])\n","  x, y = ResidualUnit(64, 256, kernel_initializer = 'he_uniform',\n","                      kernel_size = 16, dropout_rate = 0.8)([x, y])\n","  x, _ = ResidualUnit(16, 320, kernel_initializer = 'he_uniform',\n","                      kernel_size = 16, dropout_rate = 0.8)([x, y])\n","\n","  # Flatten Layer\n","  x = Flatten()(x)\n","\n","  # 2 Dense Layers\n","  x = Dense(128, activation = 'relu', kernel_initializer = 'he_normal', kernel_regularizer = l2(0.4))(x)\n","  x = Dense(64, activation = 'sigmoid', kernel_initializer = 'he_normal', kernel_regularizer = l2(0.2))(x)\n","\n","  # Output Layer\n","  diagn = Dense(4, activation = 'softmax')(x)\n","\n","  # Create Model\n","  model = Model(signal, diagn)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BR6YHx5Gqq-"},"source":["## **Model Summary**"]},{"cell_type":"code","metadata":{"id":"aGdMctLgGpWO"},"source":["# Print Model Summary (Graph With Shape)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zSpkvNOX7Dmf"},"source":["## **Callbacks, Hyperparameters and Model Compilation**"]},{"cell_type":"code","metadata":{"id":"hLTMyb4W7J1f"},"source":["if os.path.isdir(os.path.join(Path, 'Model')): # Checks If Directory Exist\n","  shutil.rmtree(os.path.join(Path, 'Model')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Model')) # Creates Directory\n","\n","# Add Different CallBacks To Model\n","callbacks = [ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1,\n","                              patience = 5, min_lr = 1e-3),\n","             CSVLogger(os.path.join(Path, 'Model', 'training.log'), append = True),\n","             ModelCheckpoint(os.path.join(Path, 'Model', 'backup_last_model.h5')),\n","             ModelCheckpoint(os.path.join(Path, 'Model', 'best_val_acc.h5'), monitor = 'val_accuracy', save_best_only = True),\n","             ModelCheckpoint(os.path.join(Path, 'Model', 'best_val_loss.h5'), monitor = 'val_loss', save_best_only = True)]\n","\n","# Define Some Parameters (Will Be Used While Training)\n","batch_size = 64\n","steps_per_epoch = int(x_train.shape[0] / batch_size)\n","validation_steps = int(x_validation.shape[0] / batch_size)\n","\n","with strategy.scope():\n","  # Compile The Model With Passing Optimizer and Loss Function\n","  model.compile(loss = 'categorical_crossentropy', optimizer = Adam(lr = 1e-3, decay = 1e-6), metrics = ['accuracy'], steps_per_execution = steps_per_epoch)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyBc3Mj9GwC6"},"source":["## **Train Model**"]},{"cell_type":"code","metadata":{"id":"xOfdF5h2G2nR"},"source":["# Training Model\n","model.fit(x_train, y_train, initial_epoch = 0, epochs = 100, batch_size = batch_size, validation_batch_size = batch_size, \n","          validation_data = (x_validation, y_validation), steps_per_epoch = steps_per_epoch, validation_steps = validation_steps, callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmI2LaYszaj4"},"source":["## **Load Best Accuracy Model**"]},{"cell_type":"code","metadata":{"id":"ZG7dhP4xzeYa"},"source":["# Loads Model\n","model = load_model(os.path.join(Path, 'Model', 'best_val_acc.h5'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjdQSAztHNvg"},"source":["## **Accuracy on Best Accuracy Model**"]},{"cell_type":"code","metadata":{"id":"dM8n_j3gzBuy"},"source":["# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Evaluate Model (Accuracy And Loss)\n","loss, acc = model.evaluate(x, y)\n","print('Loss on Training Data : ', loss)\n","print('Accuracy on Training Data :', '{:.4%}'.format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DJmL0vThoAk"},"source":["# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature[:100]]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Evaluate Model (Accuracy And Loss)\n","loss, acc = model.evaluate(x, y)\n","print('Loss on Training Data (100 Samples of Each Class) : ', loss)\n","print('Accuracy on Training Data (100 Samples of Each Class) :', '{:.4%}'.format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPJi6G92iVm5"},"source":["# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Validation_Data) for Sample in Feature]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Evaluate Model (Accuracy And Loss)\n","loss, acc = model.evaluate(x, y)\n","print('Loss on Validation Data : ', loss)\n","print('Accuracy on Validation Data :', '{:.4%}'.format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rfZJT9LhRDeD"},"source":["## **Load Best Loss Model**"]},{"cell_type":"code","metadata":{"id":"kn9udY49RFmc"},"source":["# Loads Model\n","model = load_model(os.path.join(Path, 'Model', 'best_val_loss.h5'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h6Ti3pEA-TDH"},"source":["## **Accuracy on Best Loss Model**"]},{"cell_type":"code","metadata":{"id":"2VoU9n4vzEiw"},"source":["# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Evaluate Model (Accuracy And Loss)\n","loss, acc = model.evaluate(x, y)\n","print('Loss on Training Data : ', loss)\n","print('Accuracy on Training Data :', '{:.4%}'.format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1QuREzJQ2Fm"},"source":["# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature[:100]]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Evaluate Model (Accuracy And Loss)\n","loss, acc = model.evaluate(x, y)\n","print('Loss on Training Data (100 Samples of Each Class) : ', loss)\n","print('Accuracy on Training Data (100 Samples of Each Class) :', '{:.4%}'.format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Eg5VxjYQ2GB"},"source":["# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Validation_Data) for Sample in Feature]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Evaluate Model (Accuracy And Loss)\n","loss, acc = model.evaluate(x, y)\n","print('Loss on Validation Data : ', loss)\n","print('Accuracy on Validation Data :', '{:.4%}'.format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5vWho4UqOOo3"},"source":["## **Figures**"]},{"cell_type":"code","metadata":{"id":"CKap-qpgNCSa"},"source":["plt.style.use('seaborn') # Style To Use For Matplotlib Plots\n"," \n","Time = [i / 500 for i in range(2800)]\n","Dict = {0: 'Normal', 1: 'LBBB', 2: 'RBBB', 3: 'PVC'}\n","Labels = ['DI','DII','DIII','AVR','AVL','AVF','V1','V2','V3','V4','V5','V6']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zKHDJXPcz5ec"},"source":["if os.path.isdir(os.path.join(Path, 'Training Figures (Lead II)')): # Checks If Directory Exist\n","    shutil.rmtree(os.path.join(Path, 'Training Figures (Lead II)')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Training Figures (Lead II)')) # Creates Directory\n","\n","Count = 0    \n","for Label, Feature in tqdm.tqdm(enumerate(Training_Data), unit_scale = True, miniters = 1, desc = 'Plotting Training Data (Lead II) '): # Progress Bar\n","  for Sample in Feature[:100]:\n","    df = pd.DataFrame(Sample)\n","\n","    # Plot And Save\n","    fig, ax = plt.subplots(1)\n","    fig.set_size_inches(37.33, 11.5)\n","\n","    ax.plot(Time, df.iloc[:, 1], 'b', label = Labels[1])\n","    ax.legend(loc = 'upper right', prop = {'size': 18})\n","    ax.set_xlabel('Time', fontsize = 18)\n","    ax.set_ylabel('mV', fontsize = 18)  \n","    ax.yaxis.label.set_color('black')\n","    ax.xaxis.label.set_color('black')\n","    ax.tick_params(axis = 'both', colors = 'black', labelsize = 16)\n","\n","    fig.tight_layout()\n","    fig.suptitle(Dict[Label], fontsize = 22)\n","    fig.subplots_adjust(top = 0.96, right = 0.978)\n","    fig.savefig(os.path.join(Path, 'Training Figures (Lead II)', f'Figure {Count}.png'), dpi = 300)\n","    plt.close(fig)\n"," \n","    Count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sb2y0nvMOOo5"},"source":["if os.path.isdir(os.path.join(Path, 'Training Figures')): # Checks If Directory Exist\n","    shutil.rmtree(os.path.join(Path, 'Training Figures')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Training Figures')) # Creates Directory\n","\n","Count = 0    \n","for Label, Feature in tqdm.tqdm(enumerate(Training_Data), unit_scale = True, miniters = 1, desc = 'Plotting Training Data '): # Progress Bar\n","  for Sample in Feature[:100]:\n","    df = pd.DataFrame(Sample)\n"," \n","    # Plot And Save\n","    fig, ax = plt.subplots(6, 2, sharex = True)\n","    fig.set_size_inches(37.33, 21)\n","    for i in range(2):\n","      for j in range(6):\n","        ax[j][i].plot(Time, df.iloc[:, i * 6 + j], 'b', label = Labels[i * 6 + j])\n","        ax[j][i].legend(loc = 'upper right', prop = {'size': 14})\n","        ax[j][i].set_ylabel('mV', fontsize = 12)  \n","        ax[j][i].yaxis.label.set_color('black')\n","        ax[j][i].tick_params(axis = 'both', colors = 'black')\n","      ax[j][i].set_xlabel('Time', fontsize = 12)\n","      ax[j][i].xaxis.label.set_color('black')\n","    fig.tight_layout()\n","    fig.suptitle(Dict[Label], fontsize = 18)\n","    fig.subplots_adjust(top = 0.96, right = 0.978)\n","    fig.savefig(os.path.join(Path, 'Training Figures', f'Figure {Count}.png'), dpi = 300)\n","    plt.close(fig)\n"," \n","    Count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DiWb9tjR3E73"},"source":["if os.path.isdir(os.path.join(Path, 'Validation Figures (Lead II)')): # Checks If Directory Exist\n","    shutil.rmtree(os.path.join(Path, 'Validation Figures (Lead II)')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Validation Figures (Lead II)')) # Creates Directory\n","\n","Count = 0    \n","for Label, Feature in tqdm.tqdm(enumerate(Validation_Data), unit_scale = True, miniters = 1, desc = 'Plotting Validation Data (Lead II) '): # Progress Bar\n","  for Sample in Feature[:100]:\n","    df = pd.DataFrame(Sample)\n","\n","    # Plot And Save\n","    fig, ax = plt.subplots(1)\n","    fig.set_size_inches(37.33, 11.5)\n","\n","    ax.plot(Time, df.iloc[:, 1], 'b', label = Labels[1])\n","    ax.legend(loc = 'upper right', prop = {'size': 18})\n","    ax.set_xlabel('Time', fontsize = 18)\n","    ax.set_ylabel('mV', fontsize = 18)  \n","    ax.yaxis.label.set_color('black')\n","    ax.xaxis.label.set_color('black')\n","    ax.tick_params(axis = 'both', colors = 'black', labelsize = 16)\n","\n","    fig.tight_layout()\n","    fig.suptitle(Dict[Label], fontsize = 22)\n","    fig.subplots_adjust(top = 0.96, right = 0.978)\n","    fig.savefig(os.path.join(Path, 'Validation Figures (Lead II)', f'Figure {Count}.png'), dpi = 300)\n","    plt.close(fig)\n"," \n","    Count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9HS4SugMtSc"},"source":["if os.path.isdir(os.path.join(Path, 'Validation Figures')): # Checks If Directory Exist\n","  shutil.rmtree(os.path.join(Path, 'Validation Figures')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Validation Figures')) # Creates Directory\n"," \n","Count = 0\n","for Label, Feature in tqdm.tqdm(enumerate(Validation_Data), unit_scale = True, miniters = 1, desc = 'Plotting Validation Data '): # Progress Bar\n","  for Sample in Feature:\n","    df = pd.DataFrame(Sample)\n"," \n","    # Plot And Save\n","    fig, ax = plt.subplots(6, 2, sharex = True)\n","    fig.set_size_inches(37.33, 21)\n","    for i in range(2):\n","      for j in range(6):\n","        ax[j][i].plot(Time, df.iloc[:, i * 6 + j], 'b', label = Labels[i * 6 + j])\n","        ax[j][i].legend(loc = 'upper right', prop = {'size': 14})\n","        ax[j][i].set_ylabel('mV', fontsize = 12)  \n","        ax[j][i].yaxis.label.set_color('black')\n","        ax[j][i].tick_params(axis = 'both', colors = 'black')\n","      ax[j][i].set_xlabel('Time', fontsize = 12)\n","      ax[j][i].xaxis.label.set_color('black')\n","    fig.tight_layout()\n","    fig.suptitle(Dict[Label], fontsize = 18)\n","    fig.subplots_adjust(top = 0.96, right = 0.978)\n","    fig.savefig(os.path.join(Path, 'Validation Figures', f'Figure {Count}.png'), dpi = 300)\n","    plt.close(fig)\n","\n","    Count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B4JRF1sSIi2c"},"source":["## **Predictions**"]},{"cell_type":"code","metadata":{"id":"XcUcEefspHAq"},"source":["Dict = {0: 'Normal', 1: 'LBBB', 2: 'RBBB', 3: 'PVC'}\n"," \n","# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature[:100]]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n"," \n","# Take Predictions\n","Names = [f'Figure {index}' for index in range(x.shape[0])]\n","y_pred = [Dict[Label] for Label in np.argmax(model.predict(x), axis = 1)]\n","y_true = [Dict[Label] for Label in np.argmax(y, axis = 1)]\n","\n","# Push Results To CSV File\n","Results = pd.DataFrame(list(zip(Names, y_true, y_pred)), columns = ['Name', 'Actual', 'Prediction'])\n","Results.to_csv(os.path.join(Path, 'Training Results.csv'), index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8KVzCgYuefa4"},"source":["Dict = {0: 'Normal', 1: 'LBBB', 2: 'RBBB', 3: 'PVC'}\n"," \n","# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Validation_Data) for Sample in Feature]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n","\n","# Take Predictions\n","Names = [f'Figure {index}' for index in range(x.shape[0])]\n","y_pred = [Dict[Label] for Label in np.argmax(model.predict(x), axis = 1)]\n","y_true = [Dict[Label] for Label in np.argmax(y, axis = 1)]\n"," \n","# Push Results To CSV File\n","Results = pd.DataFrame(list(zip(Names, y_true, y_pred)), columns = ['Name', 'Actual', 'Prediction'])\n","Results.to_csv(os.path.join(Path, 'Validation Results.csv'), index = False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dp7L-fTs1nGA"},"source":["## **Save Samples Into CSV File**"]},{"cell_type":"code","metadata":{"id":"T--B3qC31rob"},"source":["if os.path.isdir(os.path.join(Path, 'Training Samples')): # Checks If Directory Exist\n","  shutil.rmtree(os.path.join(Path, 'Training Samples')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Training Samples')) # Creates Directory\n","\n","# Push Samples To CSV File\n","Count = 0    \n","for Label, Feature in enumerate(Training_Data):\n","  for Sample in Feature[:100]:\n","    pd.DataFrame(Sample, columns = ['DI','DII','DIII','AVR','AVL','AVF','V1','V2','V3','V4','V5','V6']).to_csv(os.path.join(Path, 'Training Samples', f'Figure {Count}.csv'), index = False)\n","    Count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9a5N5qLuzGNs"},"source":["if os.path.isdir(os.path.join(Path, 'Validation Samples')): # Checks If Directory Exist\n","  shutil.rmtree(os.path.join(Path, 'Validation Samples')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Validation Samples')) # Creates Directory\n","\n","# Push Samples To CSV File\n","Count = 0    \n","for Label, Feature in enumerate(Validation_Data):\n","  for Sample in Feature:\n","    pd.DataFrame(Sample, columns = ['DI','DII','DIII','AVR','AVL','AVF','V1','V2','V3','V4','V5','V6']).to_csv(os.path.join(Path, 'Validation Samples', f'Figure {Count}.csv'), index = False)\n","    Count += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVC95auojZU2"},"source":["## **Confusion Matrix, Accuracy, Misclassification Rate, Precision, Recall, Specificity, F1-Score**"]},{"cell_type":"code","metadata":{"id":"GcAgmNvH0A7w"},"source":["Dict = {0: 'Normal', 1: 'LBBB', 2: 'RBBB', 3: 'PVC'}\n","\n","# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature[:100]]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n","\n","# Take Predictions\n","y_pred = [Dict[Label] for Label in np.argmax(model.predict(x), axis = 1)]\n","y_true = [Dict[Label] for Label in np.argmax(y, axis = 1)]\n","\n","Matrix = confusion_matrix(y_true, y_pred)\n","\n","Info_Dict = {'Accuracy': [],\n","             'Misclassification Rate': [],\n","             'Precision': [],\n","             'Recall': [],\n","             'Specificity': [],\n","             'F1-Score': []}\n","\n","# Calculate TP, TN, FP, FN\n","for i in range(4):\n","  FN = 0\n","  FP = 0\n","  TN = 0\n","  TP = Matrix[i][i]\n"," \n","  for j in range(4):\n","    if i!=j:\n","      FP += Matrix[i][j]\n","      FN += Matrix[j][i]\n"," \n","  for j in range(4):\n","    if i!=j:\n","      for k in range(4):\n","        if i!=k:\n","          TN += Matrix[j][k]\n","  \n","  Info_Dict['Accuracy'].append((TP + TN) / (TP + TN + FP + FN))\n","  Info_Dict['Misclassification Rate'].append((FP + FN) / (TP + TN + FP + FN))\n","  Info_Dict['Precision'].append(TP / (TP + FP))\n","  Info_Dict['Recall'].append(TP / (TP + FN))\n","  Info_Dict['Specificity'].append(TN / (TN + FP))\n","  Info_Dict['F1-Score'].append(2 * ((TP / (TP + FP)) * (TP / (TP + FN))) / ((TP / (TP + FP)) + (TP / (TP + FN))))\n","\n","display(pd.DataFrame(Matrix, index = Dict.values(), columns = Dict.values()))\n","display(pd.DataFrame(Info_Dict, index = Dict.values()).round(4))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m7xuaIy8jCG7"},"source":["Dict = {0: 'Normal', 1: 'LBBB', 2: 'RBBB', 3: 'PVC'}\n"," \n","# Separates Target Variable\n","Data = [[Sample, index] for index, Feature in enumerate(Validation_Data) for Sample in Feature]\n","x, y = np.array([Sample[0] for Sample in Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Data], dtype = np.int64))\n","\n","# Take Predictions\n","y_pred = [Dict[Label] for Label in np.argmax(model.predict(x), axis = 1)]\n","y_true = [Dict[Label] for Label in np.argmax(y, axis = 1)]\n","\n","Matrix = confusion_matrix(y_true, y_pred)\n","\n","Info_Dict = {'Accuracy': [],\n","             'Misclassification Rate': [],\n","             'Precision': [],\n","             'Recall': [],\n","             'Specificity': [],\n","             'F1-Score': []}\n"," \n","# Calculate TP, TN, FP, FN\n","for i in range(4):\n","  FN = 0\n","  FP = 0\n","  TN = 0\n","  TP = Matrix[i][i]\n"," \n","  for j in range(4):\n","    if i!=j:\n","      FP += Matrix[i][j]\n","      FN += Matrix[j][i]\n"," \n","  for j in range(4):\n","    if i!=j:\n","      for k in range(4):\n","        if i!=k:\n","          TN += Matrix[j][k]\n","  \n","  Info_Dict['Accuracy'].append((TP + TN) / (TP + TN + FP + FN))\n","  Info_Dict['Misclassification Rate'].append((FP + FN) / (TP + TN + FP + FN))\n","  Info_Dict['Precision'].append(TP / (TP + FP))\n","  Info_Dict['Recall'].append(TP / (TP + FN))\n","  Info_Dict['Specificity'].append(TN / (TN + FP))\n","  Info_Dict['F1-Score'].append(2 * ((TP / (TP + FP)) * (TP / (TP + FN))) / ((TP / (TP + FP)) + (TP / (TP + FN))))\n","\n","display(pd.DataFrame(Matrix, index = Dict.values(), columns = Dict.values()))\n","display(pd.DataFrame(Info_Dict, index = Dict.values()).round(4))"],"execution_count":null,"outputs":[]}]}