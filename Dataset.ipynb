{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Dataset.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOx8ywNx0lDxsAyU8ShqS+q"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"kGL_AUoQsQqV"},"source":["## **Load Libraries**"]},{"cell_type":"code","metadata":{"id":"A6Ceca90rcU8"},"source":["!pip install zipfile38\n"," \n","import os\n","import tqdm\n","import math\n","import random\n","import pickle\n","import shutil\n","import numpy as np\n","import pandas as pd\n","import zipfile38 as zipfile\n"," \n","from scipy import io\n","from urllib import request\n","from tensorflow.keras.utils import to_categorical"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ScSwq82XsXMn"},"source":["## **Mount Drive**"]},{"cell_type":"code","metadata":{"id":"ThDBRQU0sVRQ"},"source":["from google.colab import drive\n","drive.mount('/content/drive') # Mounts Drive In Content Directory (Under Files Section)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MBuDI6aFsZxQ"},"source":["Path = r'/content/drive/MyDrive/ECG ML'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BUT2FSDeL1VN"},"source":["## **Download Dataset**"]},{"cell_type":"code","metadata":{"id":"ySS5b8nBzbY_"},"source":["# Progress Bar For Downloading\n","def my_hook(t): \n","    '''Wraps tqdm instance.\n","    Don't forget to close() or __exit__()\n","    the tqdm instance once you're done with it (easiest using `with` syntax).\n","    Example\n","    -------\n","    >>> with tqdm(...) as t:\n","    ...     reporthook = my_hook(t)\n","    ...     urllib.urlretrieve(..., reporthook = reporthook)\n","    '''\n","    last_b = [0]\n"," \n","    def update_to(b = 1, bsize = 1, tsize = None):\n","        '''\n","        b  : int, optional\n","            Number of blocks transferred so far [default: 1].\n","        bsize  : int, optional\n","            Size of each block (in tqdm units) [default: 1].\n","        tsize  : int, optional\n","            Total size (in tqdm units). If [default: None] or -1,\n","            remains unchanged.\n","        '''\n","        if tsize not in (None, -1):\n","            t.total = tsize\n","        displayed = t.update((b - last_b[0]) * bsize)\n","        last_b[0] = b\n","        return displayed\n"," \n","    return update_to"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAH05jVoL50h"},"source":["if os.path.isdir(os.path.join(Path, 'Dataset')): # Checks If Directory Exist\n","  shutil.rmtree(os.path.join(Path, 'Dataset')) # Removes Directory\n","os.mkdir(os.path.join(Path, 'Dataset')) # Creates Directory\n","\n","# Downloads Dataset\n","for index in range(3):\n","  with tqdm.tqdm(unit = 'B', unit_scale = True, unit_divisor = 1024, miniters = 1, desc = f'Downloading TrainingSet{index + 1}.zip ') as t: # Progress Bar\n","    request.urlretrieve(f'http://hhbucket.oss-cn-hongkong.aliyuncs.com/TrainingSet{index + 1}.zip', os.path.join(Path, 'Dataset', f'TrainingSet{index + 1}.zip'), my_hook(t)) # Connects And Downloads Data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F04Fm2lsuH4V"},"source":["## **Unpack Dataset**"]},{"cell_type":"code","metadata":{"id":"gnmVkWjFNfU1"},"source":["# Unzip Archive Files\n","for Archive in ['TrainingSet1.zip', 'TrainingSet2.zip', 'TrainingSet3.zip']:\n","  with zipfile.ZipFile(os.path.join(Path, 'Dataset', Archive)) as zf:\n","    for member in tqdm.tqdm(zf.infolist(), unit_scale = True, miniters = 1, desc = f'Unpacking {Archive} '): # Progress Bar\n","      zf.extract(member, os.path.join(Path, 'Dataset'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"njbuARRBuuoI"},"source":["## **Convert and Merge Dataset**"]},{"cell_type":"code","metadata":{"id":"j_pTxErTRejo"},"source":["os.mkdir(os.path.join(Path, 'Dataset', 'Converted')) # Create Directory\n"," \n","for TrainingSet in ['TrainingSet1', 'TrainingSet2', 'TrainingSet3']:\n","  for File_Name in tqdm.tqdm(os.listdir(os.path.join(Path, 'Dataset', TrainingSet)), unit_scale = True, miniters = 1, desc = f'Converting {TrainingSet} '):\n","    if File_Name[0] == 'A':\n","      pd.DataFrame(io.loadmat(os.path.join(Path, 'Dataset', TrainingSet, File_Name))['ECG'][0][0][2].T, columns = ['DI', 'DII', 'DIII', 'AVR', 'AVL', 'AVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']).to_csv(os.path.join(Path, 'Dataset', 'Converted', f'{os.path.splitext(File_Name)[0]}.csv'), index = False) # Reads MATLAB Files And Converts Them To CSV File\n","shutil.copyfile(os.path.join(Path, 'Dataset', 'TrainingSet3', 'REFERENCE.csv'), os.path.join(Path, 'Dataset', 'Converted', 'Reference.csv')) # Copy File (Reference.csv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mPLY9GnGNTuB"},"source":["## **Select Required Features and Split Dataset**"]},{"cell_type":"code","metadata":{"id":"qcv6IYdmEwOO"},"source":["# Features To Include In Model\n","Feature_List = [1, 4, 5, 7]\n","Feature_List.sort()\n"," \n","Dict = {}\n","for index, Feature in enumerate(Feature_List):\n","  Dict[Feature] = index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PbaSsjO01Pfi"},"source":["# Reads Reference.csv File\n","df = pd.read_csv(os.path.join(Path, 'Dataset', 'Converted', 'Reference.csv'), index_col = 'Recording').sort_index()\n","\n","# Selects Required Features\n","First_label = df[df['First_label'].isin(Feature_List)].drop(columns = ['Second_label', 'Third_label'])\n","First_label.columns = ['Label']\n"," \n","Second_label = df[df['Second_label'].isin(Feature_List)].drop(columns = ['First_label', 'Third_label'])\n","Second_label = Second_label[np.logical_not(Second_label.index.isin(First_label.index))]\n","Second_label.columns = ['Label']\n"," \n","Third_label = df[df['Third_label'].isin(Feature_List)].drop(columns = ['First_label', 'Second_label'])\n","Third_label = Third_label[np.logical_not(Third_label.index.isin(pd.Series(np.concatenate([First_label.index, Second_label.index]))))]\n","Third_label.columns = ['Label']\n"," \n","# Sort Dataset According To Number Of Samples\n","df = pd.concat([First_label, Second_label, Third_label], axis = 0).convert_dtypes().sort_index()\n","Length_Dict = {File_Name: np.genfromtxt(os.path.join(Path, 'Dataset', 'Converted', f'{File_Name}.csv'), dtype = np.float64, delimiter = ',', skip_header = 1).shape[0] for File_Name in tqdm.tqdm(df.index, unit_scale = True, miniters = 1, desc = 'Calculating Lengths ')}\n","\n","df = df.reindex(sorted(df.index, key = lambda File_Name: Length_Dict[File_Name]))\n","Max = pd.read_csv(os.path.join(Path, 'Dataset', 'Converted', f'{df.index[-1]}.csv')).shape[0] # Gets The Maximum Number Of Samples Available In Any Single File"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUZOOeCevaKK"},"source":["# Separate Validation Files\n","Count = [0] * len(Feature_List)\n","Validation_Files = [[] for _ in range(len(Feature_List))]\n","\n","index = 0\n","while sum(Count) != 100:\n","  File_Name = df.index[index]\n","  Label = df.loc[File_Name, 'Label']\n","  \n","  if Count[Dict[Label]] < 25:\n","    Validation_Files[Dict[Label]].append(File_Name)\n","    Count[Dict[Label]] += 1\n"," \n","  index += 1\n"," \n","Validation_Files = [File_Name for Feature in Validation_Files for File_Name in Feature]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5CG6jboB-SVT"},"source":["# Separate Dataframe\n","train_df = df[np.logical_not(df.index.isin(Validation_Files))]\n","validation_df = df[df.index.isin(Validation_Files)]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFH4qdi0QgVu"},"source":["## **Augment Dataset**"]},{"cell_type":"code","metadata":{"id":"XFkJS61bkBa3"},"source":["# Augment Dataset Using Fixed Overlapping Window\n","\n","Max = int(Max / 1400) - 1\n","Augmented_Data = [[] for _ in range(len(Feature_List))]\n","Count = [[] for _ in range(len(Feature_List))]\n","\n","for File_Name in tqdm.tqdm(train_df.index, unit_scale = True, miniters = 1, desc = 'Loading Training Data '): # Progress Bar\n","  File = np.genfromtxt(os.path.join(Path, 'Dataset', 'Converted', f'{File_Name}.csv'), dtype = np.float64, delimiter = ',', skip_header = 1) # Read Files\n","  Label = train_df.loc[File_Name, 'Label']\n","  Length = int(File.shape[0] / 1400) - 1\n"," \n","  # Augment\n","  File_Data = [File[index * 1400:(index + 2) * 1400] for index in range(0, Length, 2)]\n","  File_Data.extend([np.empty([0]) for _ in range(int(Max / 2) - math.ceil(Length / 2))])\n","  File_Data.extend([File[index * 1400:(index + 2) * 1400] for index in range(1, Length, 2)])\n","  File_Data.extend([np.empty([0]) for _ in range(int(Max / 2) - math.floor(Length / 2))])\n","  \n","  Count[Dict[Label]].append(math.ceil(Length / 2) + math.floor(Length / 2))  \n","  Augmented_Data[Dict[Label]].append(File_Data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BnHNruzaQLS1"},"source":["# Load Validation Data\n","\n","Validation_Data = [[] for _ in range(len(Feature_List))]\n"," \n","for File_Name in tqdm.tqdm(validation_df.index, unit_scale = True, miniters = 1, desc = 'Loading Validation Data '): # Progress Bar\n","  File = np.genfromtxt(os.path.join(Path, 'Dataset', 'Converted', f'{File_Name}.csv'), dtype = np.float64, delimiter = ',', skip_header = 1)\n","  Label = validation_df.loc[File_Name, 'Label']\n","  \n","  # Append Directly\n","  Validation_Data[Dict[Label]].append(File[0:2800])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f7LfdUH6DZT0"},"source":["## **Balance Dataset**"]},{"cell_type":"code","metadata":{"id":"FD2qtfaYrDPs"},"source":["# This Balances Dataset (Equal Number Of Samples Of Each Class)\n","\n","Feature_List = [index for index in range(len(Augmented_Data))]\n","Max = min(sum(Feature) for Feature in Count) # Min Number Of Samples Present In Any Class\n","\n","Count = [0] * len(Feature_List)\n","LENGTHS = [len(Feature) for Feature in Augmented_Data]\n","Training_Data = [[] for _ in range(len(Feature_List))]\n","\n","# Main Loop\n","index = 0\n","Length = len(Feature_List)\n","while Length:\n","  Feature = Augmented_Data[Feature_List[index]]\n"," \n","  INDEX = 0\n","  LENGTH = LENGTHS[Feature_List[index]]\n","  while INDEX < LENGTH:\n","    File = Feature[INDEX]\n","    \n","    if File[0].shape[0] != 0:\n","      Training_Data[Feature_List[index]].append(File[0])  \n","      Count[Feature_List[index]] += 1\n","      if Count[Feature_List[index]] == Max:\n","        break\n","    \n","    del File[0]     \n","    if not len(File):\n","      del Feature[INDEX]\n","      INDEX -= 1\n","      LENGTH -= 1\n","\n","    INDEX += 1\n"," \n","  LENGTHS[Feature_List[index]] = LENGTH\n"," \n","  if Count[Feature_List[index]] == Max:\n","    del Feature_List[index]\n","    index -= 1\n","    Length -= 1\n"," \n","  index += 1\n"," \n","  if index == Length:\n","    index = 0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6maWY0DA-hm"},"source":["## **Save Processed Dataset**"]},{"cell_type":"code","metadata":{"id":"-ZHb2bRTA96B"},"source":["# Save Data List In Pickle Format\n","pickle_out = open(os.path.join(Path, 'Dataset', 'Training_Data.pickle'), 'wb')\n","pickle.dump(Training_Data, pickle_out)\n","pickle_out.close()\n","\n","pickle_out = open(os.path.join(Path, 'Dataset', 'Validation_Data.pickle'), 'wb')\n","pickle.dump(Validation_Data, pickle_out)\n","pickle_out.close()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tTY9UwXHvEfq"},"source":["## **Shuffle Dataset**"]},{"cell_type":"code","metadata":{"id":"cgmr3bF8ht8J"},"source":["# Restructure Data List\n","Training_Data = [[Sample, index] for index, Feature in enumerate(Training_Data) for Sample in Feature]\n","\n","# Shuffle Data\n","random.shuffle(Training_Data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1QXWyvdyiReJ"},"source":["# Restructure Data List\n","Validation_Data = [[Sample, index] for index, Feature in enumerate(Validation_Data) for Sample in Feature]\n","\n","# Shuffle Data\n","random.shuffle(Validation_Data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gyEI60nQvQ1I"},"source":["## **Save Shuffled Dataset**"]},{"cell_type":"code","metadata":{"id":"5IPIAXTAXMjp"},"source":["# Separate Target Variable\n","x_train, y_train = np.array([Sample[0] for Sample in Training_Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Training_Data], dtype = np.int64))\n","x_validation, y_validation = np.array([Sample[0] for Sample in Validation_Data], dtype = np.float64), to_categorical(np.array([Sample[1] for Sample in Validation_Data], dtype = np.int64))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPMg6DDTukiK"},"source":["# Save Data List In Pickle Format\n","pickle_out = open(os.path.join(Path, 'Dataset', 'x_train.pickle'), 'wb')\n","pickle.dump(x_train, pickle_out)\n","pickle_out.close()\n"," \n","pickle_out = open(os.path.join(Path, 'Dataset', 'y_train.pickle'), 'wb')\n","pickle.dump(y_train, pickle_out)\n","pickle_out.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h2kL0XiQRDxE"},"source":["# Save Data List In Pickle Format\n","pickle_out = open(os.path.join(Path, 'Dataset', 'x_validation.pickle'), 'wb')\n","pickle.dump(x_validation, pickle_out)\n","pickle_out.close()\n"," \n","pickle_out = open(os.path.join(Path, 'Dataset', 'y_validation.pickle'), 'wb')\n","pickle.dump(y_validation, pickle_out)\n","pickle_out.close()"],"execution_count":null,"outputs":[]}]}